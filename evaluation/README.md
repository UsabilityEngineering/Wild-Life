# Usability Evaluation

From our heuristic evaluations and cognitive walkthroughs, we found that our app had heuristic scores ranging from 2 to 4, with the lowest scores being in error recovery and help documentation. During the cognitive walkthrough, the testers had chosen the persona of Marcel which highlighted user error due to either blurry pictures or low lighting. When making our prototype and conducting our final tests, we concentrated on making the app more error tolerant and intuitive to use.

[Protocol](Protocol_Tasks.pdf)

[Prototype](https://xd.adobe.com/view/bd267d9b-a3b6-4657-a680-dda580be009d-e875/)

The tasks are designed to determine how usable the users found the app, how fast they were able to complete the task, as well as how trustworthy they found the result. Task one was simply for the user to determine what the plant in front of them was. This would convey how efficient the app design is and if the user is able to quickly learn how to use the app. Task two was similar, but there were instead three plants that the tester had to identify, one of which was harmful. This task was designed to test the memorability of the app as well as the error tolerance now that they had performed the task once. Both tasks were then given a rating from one to five on how usable the tester found the app with one being very easy and five being very hard. The app is also given a trustworthiness rating from one (not at all) to five (very) to see if the user trusted the plant results that the AI matched. 

Our overall approach to the study was “think out loud” as we felt that using that method would convey the usability of our app the best. We encouraged the testers to talk through their thinking process while completing the tasks which would inform us of any difficulties or confusion they were having with the design. This method also highlighted any common issues among testers which would allow us to redesign those parts of the app. 

The data collected conveys how well the app was designed. By taking the results of the usability ratings for both tasks, we were able to evaluate how well the app did overall. Since we asked questions after the tasks were completed, we were able to gain more insight into what could be improved. For example, one tester found the icon buttons that lead to the camera roll for photo upload and past results confusing until he clicked on them. This tells us that the icons could be better designed to represent their actions which we would be able to go back and change. Overall the results for our usability test was that most testers found the app to be really easy to use, with some minor confusion that they were able to solve on their own. 

[Data Spreadsheet](https://docs.google.com/spreadsheets/d/1YYQoZXUf8iWd1FFCkXqHMPe4IyPiy-yL4gcBVcjOV8Y/edit?usp=sharing)

